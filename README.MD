# TensorFlow to TensorRT Conversion for CIFAR-10

This project explores model format conversion, using ONNX as an intermediary, to optimize a TensorFlow model for inference with TensorRT. The model is trained on the CIFAR-10 dataset and has been tuned with various filter configurations, learning rates, and batch sizes.

## Model Architecture and Training

The final model architecture and training parameters are as follows:

*   **Convolutional Layers:**
    *   Layer 1: 64 filters, 5x5 kernel size, "same" padding.
    *   Layer 2: 128 filters, 3x3 kernel size, "same" padding.
    *   Layer 3: 128 filters, 3x3 kernel size, "same" padding.
*   **Training Parameters:**
    *   Learning Rate Reduction: Implemented using a callback with a factor of 0.2, a patience of 3 epochs (can potentially be reduced to 2), and a minimum learning rate of 0.00001.
    *   Epochs: 25 epochs. The model was also experimented with 15 and 20 epochs.

## Conversion Process (Performed in `training_and_conversion.ipynb`)

The model conversion process, up to the ONNX format, is contained within the `training_and_conversion.ipynb` notebook and involves the following steps:

1.  **TensorFlow Model Training:** The notebook contains the code to train the TensorFlow model on the CIFAR-10 dataset.
2.  **TensorFlow to ONNX:** The trained TensorFlow model (exported in TensorFlow format, *not* Keras `.h5` format) is converted to ONNX format using the `tf2onnx` tool with `opset` 16. The resulting ONNX model is saved.

## TensorRT Conversion and Inference (Performed in `tensorrt_inference.py`)

The conversion from ONNX to TensorRT and the inference process are handled in the `tensorrt_inference.py` script. This separation allows for cleaner organization and easier execution of the inference stage. The steps involved are:

1.  **ONNX to TensorRT:** The ONNX model (generated by `training_and_conversion.ipynb`) is converted to a TensorRT (`.trt`) engine using the `trtexec` command-line tool *outside* the Python script. This step requires a TensorRT version compatible with the installed CUDA version.

    ```bash
    trtexec --onnx=model.onnx --saveEngine=model.trt
    ```

    (Note: Additional `trtexec` flags may be necessary depending on the desired precision (FP16, INT8), target device, and other optimization settings. Refer to the TensorRT documentation for more details.) The `tensorrt_inference.py` script expects the `.trt` engine to be present in the same directory.

2.  **TensorRT Inference and Timing:** The `tensorrt_inference.py` script loads the generated TensorRT engine and performs inference. It also includes code to measure and report inference time, allowing for performance comparison with the original TensorFlow model (also included for comparison).

## Repository Contents

This repository contains:

*   `training_and_conversion.ipynb`: Jupyter Notebook containing the model training and ONNX conversion code.
*   `tensorrt_inference.py`: Python script containing the TensorRT engine loading, inference, and timing code.
*   [Model Files] - The converted ONNX (`.onnx`) and TensorRT (`.trt`) model files (may be included or instructions to generate them).
*   [Data] - Possibly contains the CIFAR-10 dataset or instructions on how to obtain it.

## Dependencies

*   TensorFlow
*   ONNX
*   TensorRT
*   CUDA (compatible version with TensorRT)
*   `tf2onnx`
*   Python and other common data science libraries (NumPy, etc.)

## Usage

1.  Execute the `training_and_conversion.ipynb` notebook to train the TensorFlow model and generate the ONNX model.
2.  Use the `trtexec` command-line tool (as shown above) to convert the ONNX model to a TensorRT engine.
3.  Run the `tensorrt_inference.py` script to perform inference with the TensorRT engine.
